(window.webpackJsonp=window.webpackJsonp||[]).push([[42],{542:function(t,s,a){"use strict";a.r(s);var m=a(6),c=Object(m.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("p",[a("img",{attrs:{src:"https://tva1.sinaimg.cn/large/008i3skNly1gtqjt0a9rnj61ou0pwaci02.jpg",alt:""}})]),t._v(" "),a("h2",{attrs:{id:"简述随机森林的步骤"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#简述随机森林的步骤"}},[t._v("#")]),t._v(" 简述随机森林的步骤")]),t._v(" "),a("ol",[a("li",[t._v("从原始训练数据集中，应用bootstrap方法有放回地随机抽取k个新的自助样本集，并由此构建k棵分类回归树，每次未被抽到的样本组成了Ｋ个袋外数据（out-of-bag）。")]),t._v(" "),a("li",[t._v("设有n 个特征，则在每一棵树的每个节点处随机抽取mtry个特征，通过计算每个特征蕴含的信息量，特征中选择一个最具有分类能力的特征进行节点分裂。")]),t._v(" "),a("li",[t._v("每棵树最大限度地生长， 不做任何剪裁")]),t._v(" "),a("li",[t._v("将生成的多棵树组成随机森林， 用随机森林对新的数据进行分类， 分类结果按树分类器投票多少而定。")])]),t._v(" "),a("h2",{attrs:{id:"随机森林是否会出现过拟合"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#随机森林是否会出现过拟合"}},[t._v("#")]),t._v(" 随机森林是否会出现过拟合？")]),t._v(" "),a("p",[t._v("首先什么是overfitting。模型在训练集上的表现和测试集/真实数据上的表现一般会有差异。常用的模型会在训练集上表现不错，但是真实数据上表现的好，这就是过拟合现象。是模型对数据进行了过度解读的结果，表现为模型的泛化能力。")]),t._v(" "),a("p",[t._v("在建立每一棵决策树的过程中，有两点需要注意-采样与完全分裂。首先是两个随机采样的过程，random forest对输入的数据要进行行、列的采样。对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。")]),t._v(" "),a("p",[t._v("对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那 么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。然后进行列采样，从M 个feature中，选择m个(m << M)。之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一 个分类。一般很多的决策树算法都一个重要的步骤 - 剪枝，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting。")]),t._v(" "),a("h2",{attrs:{id:"随机森林为什么不用分训练集和测试集"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#随机森林为什么不用分训练集和测试集"}},[t._v("#")]),t._v(" 随机森林为什么不用分训练集和测试集？")]),t._v(" "),a("p",[t._v("因为随机森林可以采用袋外误差（Out Of Bag Error，OOB error）来衡量随机森林的性能。假设整个数据集大小为 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"N"}})],1)],1)],1),t._v(" ，我们需要训练 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"m"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"t"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"r"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"e"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"e"}})],1)],1)],1),t._v("棵决策树。在构建每棵树时，我们都需要从原始数据集中有放回的抽样"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"N"}})],1)],1)],1),t._v("次（bootstrap抽样法），构成一个大小同为"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"N"}})],1)],1)],1),t._v("的训练集（这里的训练集指的是构建决策树的数据集，而非训练集与测试集之分），然后我们在这个新的训练集上生长出一棵决策树，同样的，对于每棵决策树都采取这样的方式直到所有决策树构建完成。")],1),t._v(" "),a("p",[t._v("对于每棵树而言，其训练集虽然大小和原始数据集相同都是"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"N"}})],1)],1)],1),t._v("，但是因为使用有放回的采样方式生成训练集，可能有些样本被抽中两次或者更多，这导致一部分数据没有出现在训练集里（当训练集足够大的时候，大约有36.8%），这部分样本称为袋外数据（Out Of Bag Data）。可想而知，每棵决策树的训练集都不一样，但正是这样才保证了每棵决策树之间的多样性。")],1),t._v(" "),a("p",[t._v("那什么是随机森林的OOB误差呢？对于每个样本来说，都有一部分决策树的训练集中没有用到这个样本，那么这些树通过多数表决产生的预测如果与类标相同，那么就是预测正确，否则预测错误。将所有N个样本进行同样的判断，统计其中错误的比例，结果就是OOB误差。")]),t._v(" "),a("p",[t._v("提出随机森林的作者在论文里证明了OOB误差对衡量随机森林性能的有效性。当然也可以划分训练集和测试集，这样可以衡量不同随机森林之间性能的差异，因为保证了测试集的相同。但是一般来说是不用划分训练集和测试集的。")]),t._v(" "),a("h2",{attrs:{id:"随机森林是如何处理缺失值的"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#随机森林是如何处理缺失值的"}},[t._v("#")]),t._v(" 随机森林是如何处理缺失值的？")]),t._v(" "),a("p",[t._v("随机森林是故统计学家Leo Breiman提出的,和 Gradient boosted\ntree一样,它的基模型是决策树。在介绍rF时, Breiman就提出两种解决缺失值的方法")]),t._v(" "),a("ul",[a("li",[t._v("方法1(快速简单但效果差):把数值型变量( numerical variables)中的缺失值用其所对应的类别中(class)的中位数( median)替换。把描述型变量(categorical variables)缺失的部分用所对应类别中出现最多的数值替代(most frequent non-missing value)。以数值型变量为例\n"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"X"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-TeXAtom",{attrs:{size:"s"}},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"i"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"j"}})],1)],1)],1)],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"="}})],1),a("mjx-mi",{staticClass:"mjx-i",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"M"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"e"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"d"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"i"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"a"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"n"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"("}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:")"}})],1),a("mjx-mstyle",[a("mjx-mspace",{staticStyle:{width:"1em"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"w"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"e"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"r"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"e"}})],1),a("mjx-mstyle",[a("mjx-mspace",{staticStyle:{width:"1em"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"k"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"="}})],1),a("mjx-mn",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"1"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mn",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"2"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mn",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"3"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"22EF"}})],1),a("mjx-mi",{staticClass:"mjx-i",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"n"}})],1),a("mjx-mstyle",[a("mjx-mspace",{staticStyle:{width:"1em"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"a"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"n"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"d"}})],1),a("mjx-mstyle",[a("mjx-mspace",{staticStyle:{width:"1em"}})],1),a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"X"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-TeXAtom",{attrs:{size:"s"}},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"k"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"j"}})],1)],1)],1)],1)],1)],1),t._v(" is present")],1),t._v(" "),a("li",[t._v("方法2(耗时费力但效果好)：虽然依然是使用中位数和出现次数最多的数来进行替换，方法2引入了权重。即对需要替换的数据先和其他数据做相似度测量(proximity measurement)也就是下面公式中的Weight( [公式] )，在补全缺失点是相似的点的数据会有更高的权重W。以数值型变量为例：")])]),t._v(" "),a("p"),a("p",[a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML",display:"true"}},[a("mjx-math",{staticClass:" MJX-TEX",attrs:{display:"true"}},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"X"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-TeXAtom",{attrs:{size:"s"}},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"k"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"j"}})],1)],1)],1)],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"="}})],1),a("mjx-mfrac",{attrs:{space:"4"}},[a("mjx-frac",{attrs:{type:"d"}},[a("mjx-num",[a("mjx-nstrut",{attrs:{type:"d"}}),a("mjx-mn",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"1"}})],1)],1),a("mjx-dbox",[a("mjx-dtable",[a("mjx-line",{attrs:{type:"d"}}),a("mjx-row",[a("mjx-den",[a("mjx-dstrut",{attrs:{type:"d"}}),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"n"}})],1)],1)],1)],1)],1)],1)],1),a("mjx-munderover",{attrs:{space:"2"}},[a("mjx-over",{staticStyle:{"padding-bottom":"0.192em","padding-left":"0.919em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"n"}})],1)],1),a("mjx-box",[a("mjx-munder",[a("mjx-row",[a("mjx-base",{staticStyle:{"padding-left":"0.409em"}},[a("mjx-mo",{staticClass:"mjx-lop"},[a("mjx-c",{attrs:{c:"2211"}})],1)],1)],1),a("mjx-row",[a("mjx-under",{staticStyle:{"padding-top":"0.167em"}},[a("mjx-TeXAtom",{attrs:{size:"s"}},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"i"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"="}})],1),a("mjx-mn",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"1"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"2260"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"k"}})],1)],1)],1)],1)],1)],1)],1),a("mjx-msub",{attrs:{space:"2"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"W"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-TeXAtom",{attrs:{size:"s"}},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"i"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"k"}})],1)],1)],1)],1),a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"X"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-TeXAtom",{attrs:{size:"s"}},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"i"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"."}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"j"}})],1)],1)],1)],1),a("mjx-mstyle",[a("mjx-mspace",{staticStyle:{width:"1em"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"w"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"e"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"r"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"e"}})],1),a("mjx-mstyle",[a("mjx-mspace",{staticStyle:{width:"1em"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"k"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"="}})],1),a("mjx-mn",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"1"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mn",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"2"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mn",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"3"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"B7"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"3"}},[a("mjx-c",{attrs:{c:"B7"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"3"}},[a("mjx-c",{attrs:{c:"B7"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mi",{staticClass:"mjx-i",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"n"}})],1),a("mjx-mstyle",[a("mjx-mspace",{staticStyle:{width:"1em"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"a"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"n"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"d"}})],1),a("mjx-mstyle",[a("mjx-mspace",{staticStyle:{width:"1em"}})],1),a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"X"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-TeXAtom",{attrs:{size:"s"}},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"i"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"j"}})],1)],1)],1)],1),a("mjx-mstyle",[a("mjx-mspace",{staticStyle:{width:"1em"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"i"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"s"}})],1),a("mjx-mstyle",[a("mjx-mspace",{staticStyle:{width:"1em"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"p"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"r"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"e"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"s"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"e"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"n"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"t"}})],1)],1)],1)],1),t._v("\nBreiman说明了第二种方法的效果更好，但需要的时间更长。\n这也是为什么工具包中一般不提供数据补全的功能，因为会影响到工具包的效率。"),a("p"),t._v(" "),a("h2",{attrs:{id:"随机森林与gbdt之间的区别"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#随机森林与gbdt之间的区别"}},[t._v("#")]),t._v(" 随机森林与GBDT之间的区别")]),t._v(" "),a("ul",[a("li",[t._v("相同点")])]),t._v(" "),a("ol",[a("li",[t._v("都是由多棵树组成")]),t._v(" "),a("li",[t._v("最终的结果都是由多棵树一起决定")])]),t._v(" "),a("ul",[a("li",[t._v("不同点")])]),t._v(" "),a("ol",[a("li",[t._v("组成随机森林的树可以分类树也可以是回归树，而GBDT只由回归树组成")]),t._v(" "),a("li",[t._v("组成随机森林的树可以并行生成，而GBDT是串行生成")]),t._v(" "),a("li",[t._v("随机森林的结果是多数表决的，而GBDT则是多棵树累加之和")]),t._v(" "),a("li",[t._v("随机森林对异常值不敏感，而GBDT对异常值比较敏感")]),t._v(" "),a("li",[t._v("随机森林是通过减少模型的方差来提高性能，而GBDT是减少模型的偏差来提高性能的")])]),t._v(" "),a("h2",{attrs:{id:"随机森林与svm的比较"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#随机森林与svm的比较"}},[t._v("#")]),t._v(" 随机森林与SVM的比较")]),t._v(" "),a("ol",[a("li",[t._v("不需要调节过多的参数，因为随机森林只需要调节树的数量，而且树的数量一般是越多越好，而其他机器学习算法，比如SVM，有非常多超参数需要调整，如选择最合适的核函数，正则惩罚等。")]),t._v(" "),a("li",[t._v("分类较为简单、直接。随机森林和支持向量机都是非参数模型（复杂度随着训练模型样本的增加而增大）。相较于一般线性模型，就计算消耗来看，训练非参数模型因此更为耗时耗力。分类树越多，需要更耗时来构建随机森林模型。同样，我们训练出来的支持向量机有很多支持向量，最坏情况为，我们训练集有多少实例，就有多少支持向量。虽然，我们可以使用多类支持向量机，但传统多类分类问题的执行一般是one-vs-all（所谓one-vs-all 就是将binary分类的方法应用到多类分类中。比如我想分成K类，那么就将其中一类作为positive），因此我们还是需要为每个类训练一个支持向量机。相反，决策树与随机深林则可以毫无压力解决多类问题。")]),t._v(" "),a("li",[t._v("比较容易入手实践。随机森林在训练模型上要更为简单。你很容易可以得到一个又好且具鲁棒性的模型。随机森林模型的复杂度与训练样本和树成正比。支持向量机则需要我们在调参方面做些工作，除此之外，计算成本会随着类增加呈线性增长。")]),t._v(" "),a("li",[t._v("小数据上，SVM优异，而随机森林对数据需求较大。就经验来说，我更愿意认为支持向量机在存在较少极值的小数据集上具有优势。随机森林则需要更多数据但一般可以得到非常好的且具有鲁棒性的模型。")])]),t._v(" "),a("h2",{attrs:{id:"说一说随机森林的优缺点"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#说一说随机森林的优缺点"}},[t._v("#")]),t._v(" 说一说随机森林的优缺点")]),t._v(" "),a("p",[t._v("优点")]),t._v(" "),a("ul",[a("li",[t._v("不必担心过度拟合；")]),t._v(" "),a("li",[t._v("适用于数据集中存在大量未知特征；")]),t._v(" "),a("li",[t._v("能够估计哪个特征在分类中更重要；")]),t._v(" "),a("li",[t._v("具有很好的抗噪声能力；")]),t._v(" "),a("li",[t._v("算法容易理解；")]),t._v(" "),a("li",[t._v("可以并行处理。")])]),t._v(" "),a("p",[t._v("缺点")]),t._v(" "),a("ul",[a("li",[t._v("对小量数据集和低维数据集的分类不一定可以得到很好的效果。")]),t._v(" "),a("li",[t._v("执行速度虽然比Boosting等快，但是比单个的决策树慢很多。")]),t._v(" "),a("li",[t._v("可能会出现一些差异度非常小的树，淹没了一些正确的决策。")]),t._v(" "),a("li",[t._v("由于树是随机生成的，结果不稳定（kpi值比较大）")])])])}),[],!1,null,null,null);s.default=c.exports}}]);