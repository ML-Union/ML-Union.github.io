(window.webpackJsonp=window.webpackJsonp||[]).push([[94],{592:function(t,s,a){"use strict";a.r(s);var m=a(6),i=Object(m.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("p",[a("img",{attrs:{src:"https://tva1.sinaimg.cn/large/008i3skNly1gtqjt0a9rnj61ou0pwaci02.jpg",alt:""}})]),t._v(" "),a("h2",{attrs:{id:"请简述dssm模型的原理"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#请简述dssm模型的原理"}},[t._v("#")]),t._v(" 请简述DSSM模型的原理")]),t._v(" "),a("p",[t._v("DSSM(Deep Structured Semantic Models)也叫深度语义匹配模型，最早是微软发表的一篇应用于 NLP 领域中计算语义相似度任务的文章。")]),t._v(" "),a("p",[t._v("DSSM 深度语义匹配模型原理很简单：获取搜索引擎中的用户搜索 query 和 doc 的海量曝光和点击日志数据，训练阶段分别用复杂的深度学习网络构建 query 侧特征的 query embedding 和 doc 侧特征的 doc embedding，线上 infer 时通过计算两个语义向量的 cos 距离来表示语义相似度，最终获得语义相似模型。这个模型既可以获得语句的低维语义向量表达 sentence embedding，还可以预测两句话的语义相似度。")]),t._v(" "),a("p",[t._v("DSSM 模型总的来说可以分成三层结构，分别是输入层、表示层和匹配层。结构如下图所示：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://tva1.sinaimg.cn/large/008i3skNly1gs7c8xnq0hj316k0oggrd.jpg",alt:""}})]),t._v(" "),a("h2",{attrs:{id:"dssm的输入层将文本映射到低维向量空间转化成向量-会存在什么问题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dssm的输入层将文本映射到低维向量空间转化成向量-会存在什么问题"}},[t._v("#")]),t._v(" DSSM的输入层将文本映射到低维向量空间转化成向量 会存在什么问题？")]),t._v(" "),a("p",[t._v("NLP 领域里中英文有比较大的差异，在输入层处理方式不同。")]),t._v(" "),a("p",[t._v("英文的输入层通过 Word Hashing 方式处理，该方法基于字母的 n-gram，主要作用是减少输入向量的维度。举例说明，假如现在有个词 boy，开始和结束字符分别用#表示，那么输入就是(#boy#)。将词转化为字母 n-gram 的形式，如果设置 n 为 3，那么就能得到(#bo,boy,oy#)三组数据，将这三组数据用 n-gram 的向量来表示。")]),t._v(" "),a("p",[t._v("使用 Word Hashing 方法存在的问题是可能造成冲突。因为两个不同的词可能有相同的 n-gram 向量表示。下图是在不同的英语词典中分别使用 2-gram 和 3-gram 进行 Word Hashing 时的向量空间以及词语碰撞统计：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://tva1.sinaimg.cn/large/008i3skNly1gs7ce5jyrzj312u098tb0.jpg",alt:""}})]),t._v(" "),a("p",[t._v("可以看出在 50W 词的词典中如果使用 2-gram，也就是两个字母的粒度来切分词，向量空间压缩到 1600 维，产生冲突的词有 1192 个(这里的冲突是指两个词的向量表示完全相同)。如果使用 3-gram 向量空间压缩到 3W 维，产生冲突的词只有 22 个。综合下来论文中使用 3-gram 切分词。")]),t._v(" "),a("p",[t._v("中文输入层和英文有很大差别，首先要面临的是分词问题。如果要分词推荐 jieba 或者北大 pkuseg，不过现在很多模型已经不进行分词，比如 BERT 中文的预训练模型就直接使用单字作为最小粒度。")]),t._v(" "),a("h2",{attrs:{id:"请简要描述dssm是如何应用于召回的-结构是怎样的"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#请简要描述dssm是如何应用于召回的-结构是怎样的"}},[t._v("#")]),t._v(" 请简要描述DSSM是如何应用于召回的？结构是怎样的？")]),t._v(" "),a("p",[t._v("DSSM模型是微软2013年发表的一个关于query/ doc的相似度计算模型，后来发展成为一种所谓”双塔“的框架广泛应用于广告、推荐等领域的召回和排序问题中。网络结构如下：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://tva1.sinaimg.cn/large/007S8ZIlly1gi6las6ozrj30im08jq5m.jpg",alt:""}})]),t._v(" "),a("p",[t._v("1）首先特征输入层将Query和Doc（one-hot编码）转化为embedding向量，原论文针对英文输入还提出了一种word hashing的特殊embedding方法用来降低字典规模。我们在针对中文embedding时使用word2vec类常规操作即可；")]),t._v(" "),a("p",[t._v("2）经过embedding之后的词向量，接下来是多层DNN网络映射得到针对Query和Doc的128维语义特征向量；")]),t._v(" "),a("p",[t._v("3）最后会使用Query和Doc向量进行余弦相似度计算得到相似度R，然后进行softmax归一化得到最终的指标后验概率P，训练目标针对点击的正样本拟合P为1，否则拟合P为0；")]),t._v(" "),a("p",[t._v("DSSM 模型的最大特点就是 Query 和 Document 是两个独立的子网络，后来这一特色被移植到推荐算法的召回环节，即对用户端（User）和物品端（Item）分别构建独立的子网络塔式结构。该方式对工业界十分友好，两个子网络产生的 Embedding 向量可以独自获取及缓存。目前工业界流行的 DSSM 双塔网络结构如下图所示：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://tva1.sinaimg.cn/large/007S8ZIlly1gi6l79v8xnj30fh0ai3zx.jpg",alt:""}})]),t._v(" "),a("p",[t._v("双塔模型两侧分别对（用户，上下文）和（物品）进行建模，并在最后一层计算二者的内积。")]),t._v(" "),a("p",[t._v("其中：")]),t._v(" "),a("ul",[a("li",[a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"x"}})],1)],1)],1),t._v("为（用户，上下文）的特征，"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"y"}})],1)],1)],1),t._v("为（物品）的特征；")],1),t._v(" "),a("li",[a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"u"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"("}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"x"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v("表示（用户，上下文）最终的 Embedding 向量表示， "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"v"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"("}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"y"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v(" 表示（物品）最终的 Embedding 向量表示；")],1),t._v(" "),a("li",[a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"<"}})],1),a("mjx-mi",{staticClass:"mjx-i",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"u"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"("}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"x"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:")"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mi",{staticClass:"mjx-i",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"v"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"("}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"y"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:")"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:">"}})],1)],1)],1),t._v(" 表示（用户，上下文）和（物品）的余弦相似度。")],1)]),t._v(" "),a("p",[t._v("当模型训练完成时，物品的 Embedding 是可以保存成词表的，线上应用的时候只需要查找对应的 Embedding 即可。因此线上只需要计算 （用户，上下文） 一侧的 Embedding，基于 Annoy 或 Faiss 技术索引得到用户偏好的候选集。")]),t._v(" "),a("h2",{attrs:{id:"请简述一下dssm的优缺点"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#请简述一下dssm的优缺点"}},[t._v("#")]),t._v(" 请简述一下DSSM的优缺点")]),t._v(" "),a("p",[t._v("先说说 DSSM 模型的优点：")]),t._v(" "),a("ul",[a("li",[t._v("解决了 LSA、LDA、Autoencoder 等方法存在的字典爆炸问题，从而降低了计算复杂度。因为英文中词的数量要远远高于字母 n-gram 的数量；")]),t._v(" "),a("li",[t._v("中文方面使用字作为最细切分粒度，可以复用每个字表达的语义，减少分词的依赖，从而提高模型的泛化能力；")]),t._v(" "),a("li",[t._v("字母的 n-gram 可以更好的处理新词，具有较强的鲁棒性；")]),t._v(" "),a("li",[t._v("使用有监督的方法，优化语义 embedding 的映射问题；")]),t._v(" "),a("li",[t._v("省去了人工特征工程；")]),t._v(" "),a("li",[t._v("采用有监督训练，精度较高。传统的输入层使用 embedding 的方式(比如 Word2vec 的词向量)或者主题模型的方式(比如 LDA 的主题向量)做词映射，再把各个词的向量拼接或者累加起来。由于 Word2vec 和 LDA 都是无监督训练，会给模型引入误差。")])]),t._v(" "),a("p",[t._v("再说说 DSSM 模型的缺点：")]),t._v(" "),a("ul",[a("li",[t._v("Word Hashing 可能造成词语冲突；")]),t._v(" "),a("li",[t._v("采用词袋模型，损失了上下文语序信息。这也是后面会有 CNN-DSSM、LSTM-DSSM 等 DSSM 模型变种的原因；")]),t._v(" "),a("li",[t._v("搜索引擎的排序由多种因素决定，用户点击时 doc 排名越靠前越容易被点击，仅用点击来判断正负样本，产生的噪声较大，模型难以收敛；")]),t._v(" "),a("li",[t._v("效果不可控。因为是端到端模型，好处是省去了人工特征工程，但是也带来了端到端模型效果不可控的问题。")])]),t._v(" "),a("h2",{attrs:{id:"dssm中的负样本为什么是随机采样得到的-而不用-曝光未点击-当负样本"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dssm中的负样本为什么是随机采样得到的-而不用-曝光未点击-当负样本"}},[t._v("#")]),t._v(" DSSM中的负样本为什么是随机采样得到的，而不用“曝光未点击”当负样本？")]),t._v(" "),a("p",[t._v("召回是将用户可能喜欢的item，和用户根本不感兴趣的海量item分离开来，他面临的数据环境相对于排序来说是鱼龙混杂的。")]),t._v(" "),a("p",[t._v("所以我们希望召回训练数据的正样本是user和item匹配度最高的那些样本，也即用户点击样本，负样本是user和item最不匹配的那些样本，但不能拿“曝光未点击”作为召回模型的负样本，因为我们从线上日志获得的训练样本，已经是上一版本的召回、粗排、精排替用户筛选过的，即已经是对用户“匹配度较高”的样本了，即推荐系统以为用户会喜欢他们，但用户的行为又表明的对他的嫌弃。拿这样的样本训练出来的模型做召回，并不能与线上环境的数据分布保持一致。也就是说曝光未点击既不能成为合格的正样本，也不能成为合格的负样本。")]),t._v(" "),a("p",[t._v("所以一般的做法是拿点击样本做正样本，拿随机采样做负样本，因为线上召回时，候选库里大多数的物料是与用户没有关系的，随机抽样能够很好地模拟这一分布。")]),t._v(" "),a("hr"),t._v(" "),a("p",[a("img",{attrs:{src:"https://tva1.sinaimg.cn/large/008i3skNly1gtqjt0a9rnj61ou0pwaci02.jpg",alt:""}})])])}),[],!1,null,null,null);s.default=i.exports}}]);